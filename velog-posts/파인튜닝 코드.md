<h1 id="모델-및-토크나이저-로드">모델 및 토크나이저 로드</h1>
<pre><code class="language-python">import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer, SFTConfig
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
from unsloth.chat_templates import get_chat_template

# 1. 모델 및 토크나이저 로드
model_name = &quot;unsloth/Qwen3-30B-A3B-Instruct-2507&quot; # 사용할 사전 학습된 모델의 ID
max_seq_length = 2048  # 모델이 한 번에 처리할 수 있는 최대 토큰 길이
dtype = None  # 모델 가중치의 데이터 타입
load_in_4bit = False # 4비트 양자화 사용 여부
load_in_8bit = True # 8비트 양자화 사용 여부
full_finetuning = False # 전체 파라미터 미세 조정 여부

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
    load_in_8bit=load_in_8bit,
    full_finetuning=full_finetuning,
)

# 토크나이저 ChatML 템플릿 설정 (Unsloth 유틸리티 활용)
tokenizer = get_chat_template(
    tokenizer,
    chat_template=&quot;chatml&quot;, # ChatML 형식(IM_START/IM_END)으로 대화 템플릿 지정
)</code></pre>
<hr />
<h1 id="lora-어댑터-설정">LoRA 어댑터 설정</h1>
<pre><code class="language-python"># 2. LoRA 어댑터 설정
model = FastLanguageModel.get_peft_model(
    model,
    r=32, # LoRA의 Rank(계수). 숫자가 클수록 학습할 파라미터가 많아져 성능은 오르지만 메모리를 더 사용함 (일반적으로 8, 16, 32, 64 사용)
    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,
                    &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;], # LoRA를 적용할 모델의 레이어 목록 (모든 선형 레이어에 적용하면 성능이 가장 좋음)
    lora_alpha=8, # LoRA 업데이트의 가중치 계수. 학습률에 영향을 줌
    lora_dropout=0, # 과적합 방지를 위한 드롭아웃 비율. Unsloth는 최적화를 위해 0을 권장함
    bias=&quot;none&quot;, # 바이어스 파라미터를 학습할지 여부. &quot;none&quot;으로 설정하여 파라미터 절약
    use_gradient_checkpointing=&quot;unsloth&quot;, # 메모리 사용량을 줄이는 기술 (Unsloth 전용 최적화 옵션 사용)
    random_state=3407, # 랜덤 시드 고정
    use_rslora=False, # Rank-Stabilized LoRA 사용 여부
    loftq_config=None, # LoftQ(양자화 초기화) 설정 여부 (이미 양자화된 모델을 로드했거나 일반 LoRA를 사용할 때는 None 처리 진행)
)</code></pre>
<hr />
<h1 id="데이터셋-로드-및-포맷팅">데이터셋 로드 및 포맷팅</h1>
<pre><code class="language-python"># 3. 데이터셋 로드 및 포맷팅
dataset_file = &quot;./temp/final/deduplicated_dataset.json&quot;
try:
    dataset = load_dataset(&quot;json&quot;, data_files=dataset_file, split=&quot;train&quot;)
except FileNotFoundError:
    print(f&quot;오류: '{dataset_file}' 파일을 찾을 수 없습니다. 경로를 확인해주세요.&quot;)
    pass

# 프롬프트 템플릿 정의 (ChatML 형식 직접 지정)
# Qwen 모델이 이해하는 특수 태그(&lt;|im_start|&gt;, &lt;|im_end|&gt;)를 사용하여 만든 대화 구조
qwen_template = &quot;&quot;&quot;&lt;|im_start|&gt;system
{system_message}&lt;|im_end|&gt;
&lt;|im_start|&gt;user
{instruction}&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
{output}&lt;|im_end|&gt;&quot;&quot;&quot;

# 시스템 프롬프트 설정
# 모델에게 &quot;너는 VTW 규정 전문가야&quot;라고 역할을 부여
SYSTEM_PROMPT = &quot;당신은 VTW 회사의 취업 규정 전문 AI 어시스턴트입니다. 제공된 규정 내용을 바탕으로 질문에 정확하게 답변하세요.&quot;

def formatting_prompts_func(examples):
    # 데이터셋의 각 컬럼(질문, 근거자료, 정답)을 변수로 가져옴
    instructions = examples[&quot;question&quot;]
    inputs       = examples[&quot;source&quot;]
    outputs      = examples[&quot;answer&quot;]

    texts = []
    # zip을 사용하여 질문, 근거, 답변을 하나씩 짝지어 순회
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Context(조항/규정)가 있으면 질문 뒤에 붙여서 모델에게 근거 제공(환각 방지)
        if input:
            user_msg = f&quot;{instruction}\n\n[참고 규정]\n{input}&quot;
        else:
            user_msg = instruction

        # 위에서 정의한 qwen_template에 내용을 채워 넣습니다.
        text = qwen_template.format(
            system_message=SYSTEM_PROMPT,
            instruction=user_msg,
            output=output
        ) + tokenizer.eos_token # [매우 중요] 문장이 끝났음을 알리는 토큰(EOS)을 수동 추가

        texts.append(text)
    return { &quot;text&quot; : texts } # 처리된 텍스트를 &quot;text&quot; 컬럼으로 반환

# 데이터셋 매핑
# batched=True: 데이터를 한 번에 여러 개씩 처리할 수 있게 수정
dataset = dataset.map(formatting_prompts_func, batched=True)

# 학습/검증 데이터셋 분리 (9:1 비율)
dataset_split = dataset.train_test_split(test_size=0.1)
train_dataset = dataset_split['train']
eval_dataset = dataset_split['test']

print(f&quot;학습 데이터 개수: {len(train_dataset)}&quot;)
print(f&quot;검증 데이터 개수: {len(eval_dataset)}&quot;)</code></pre>
<hr />
<h1 id="트레이너-설정">트레이너 설정</h1>
<pre><code class="language-python">trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    eval_dataset = eval_dataset,

    args = SFTConfig(
        dataset_text_field = &quot;text&quot;, # 데이터셋에서 실제 학습할 텍스트가 담긴 컬럼 이름
        max_seq_length = max_seq_length, # 데이터 최대 길이 명시

        &quot;&quot;&quot;배치 사이즈 및 속도 관련 설정&quot;&quot;&quot;
        per_device_train_batch_size = 2, # GPU 1개당 한 번에 처리할 데이터 개수
        gradient_accumulation_steps = 4, # 가상 배치 크기( 실제 배치 = per_device_train_batch_size * gradient_accumulation_steps)

        &quot;&quot;&quot;학습 스텝 및 기간 설정&quot;&quot;&quot;
        warmup_steps = 5, # 학습 초기에 학습률을 0부터 목표치까지 서서히 올리는 단계 학습 안정화 도움
        num_train_epochs = 1, # 전체 데이터셋 학습 수
        max_steps = 30, # 최대 학습 스텝 수. 이 값이 설정되면 Epoch 설정은 무시됨
                        # 실제 전체 데이터를 학습하려면 값을 -1로 설정

        # --- 최적화 및 학습률 설정 ---
        learning_rate = 2e-4, # 학습률(Learning Rate) LoRA 학습 시 보통 2e-4를 사용
        logging_steps = 1, # 로그를 출력할 주기, 1로 설정하면 매 스텝마다 Loss 값 출력
        optim = &quot;adamw_8bit&quot;, # 8비트 AdamW 최적화 사용. 일반 AdamW보다 메모리를 훨씬 적게 사용함 (Unsloth 최적화)
        weight_decay = 0.001, # 가중치 감쇠, 모델이 너무 복잡해지는 것을 막아 과적합 방지
        lr_scheduler_type = &quot;linear&quot;, # 학습률 스케줄러

        # --- 기타 설정 ---
        seed = 3407, # 랜덤 시드
        report_to = &quot;none&quot;, # &quot;wandb&quot; 등을 설정하면 학습 그래프를 웹에서 볼 수 있음. 로컬에서는 &quot;none&quot; 사용
    ),
)</code></pre>
<hr />
<h1 id="학습-및-저장">학습 및 저장</h1>
<pre><code class="language-python"># 5. 학습 시작
print(&quot;학습을 시작합니다...&quot;)
trainer_stats = trainer.train()

# 6. 모델 저장
model.save_pretrained(&quot;lora_model&quot;)
tokenizer.save_pretrained(&quot;lora_model&quot;) # 토크나이저도 함께 저장하는 것을 권장

print(&quot;학습 및 저장이 완료되었습니다.&quot;)</code></pre>