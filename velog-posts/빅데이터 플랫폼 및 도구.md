<h1 id="6-빅데이터-플랫폼-및-도구">6. 빅데이터 플랫폼 및 도구</h1>
<h2 id="📌-1-hadoop-에코시스템">📌 1. Hadoop 에코시스템</h2>
<p>Hadoop은 단순한 분산 저장/처리 시스템(HDFS + MapReduce)을 넘어서, 다양한 관련 도구들과 함께 <strong>에코시스템</strong>을 구성합니다.</p>
<h3 id="🔹-hive">🔹 Hive</h3>
<ul>
<li>SQL처럼 데이터를 질의할 수 있게 해주는 <strong>데이터 웨어하우스 인프라</strong></li>
<li>내부적으로는 MapReduce 또는 Spark로 실행됨</li>
<li>친숙한 SQL 문법 → 대용량 데이터 처리<pre><code class="language-sql">SELECT count(*) FROM bigdata_table WHERE category = 'A';</code></pre>
</li>
</ul>
<h3 id="🔹-pig">🔹 Pig</h3>
<ul>
<li>데이터 흐름 기반의 스크립트 언어 (Pig Latin)</li>
<li>복잡한 데이터 변환, 집계 등을 간편하게 처리</li>
</ul>
<h3 id="🔹-hbase">🔹 HBase</h3>
<ul>
<li>Hadoop 기반 <strong>컬럼 지향 NoSQL 데이터베이스</strong></li>
<li>실시간 읽기/쓰기 가능 → IoT, 로그 수집 등에 적합</li>
</ul>
<h3 id="🔹-zookeeper">🔹 Zookeeper</h3>
<ul>
<li>분산 시스템의 <strong>구성, 동기화, 네이밍, 감시 역할 수행</strong></li>
<li>Kafka, HBase 등 분산 환경에서 필수 요소</li>
</ul>
<hr />
<h2 id="📌-2-spark-kafka-flume-sqoop-개요">📌 2. Spark, Kafka, Flume, Sqoop 개요</h2>
<h3 id="🔹-apache-spark">🔹 Apache Spark</h3>
<ul>
<li>인메모리 기반 <strong>고속 분산 처리 엔진</strong></li>
<li>ETL, SQL, 머신러닝, 스트리밍까지 통합 지원</li>
<li>PySpark로 Python에서 사용 가능</li>
</ul>
<h3 id="🔹-apache-kafka">🔹 Apache Kafka</h3>
<ul>
<li><strong>분산 메시징 시스템</strong></li>
<li>대용량 실시간 데이터 스트림 처리</li>
<li>Producer → Kafka Broker → Consumer 구조</li>
</ul>
<h3 id="🔹-apache-flume">🔹 Apache Flume</h3>
<ul>
<li><strong>로그/이벤트 데이터 수집 도구</strong></li>
<li>다양한 Source로부터 데이터를 수집해 HDFS, HBase로 전송</li>
</ul>
<h3 id="🔹-apache-sqoop">🔹 Apache Sqoop</h3>
<ul>
<li>RDBMS(예: MySQL, Oracle) ↔ Hadoop 간 데이터 전송 도구
```bash
sqoop import \</li>
<li>-connect jdbc:mysql://localhost/db \</li>
<li>-username root --password pass \</li>
<li>-table user_info --target-dir /hdfs/user_info<pre><code></code></pre></li>
</ul>
<hr />
<h2 id="📌-3-클라우드-기반-빅데이터-플랫폼">📌 3. 클라우드 기반 빅데이터 플랫폼</h2>
<h3 id="🔹-aws-amazon-web-services">🔹 AWS (Amazon Web Services)</h3>
<ul>
<li><strong>EMR</strong>: Hadoop/Spark 클러스터 자동 생성 및 관리</li>
<li><strong>S3</strong>: 대용량 저장소 (Data Lake 역할)</li>
<li><strong>Athena</strong>: S3 내 데이터에 SQL 질의</li>
<li><strong>Glue</strong>: ETL 파이프라인 구축</li>
</ul>
<h3 id="🔹-gcp-google-cloud-platform">🔹 GCP (Google Cloud Platform)</h3>
<ul>
<li><strong>BigQuery</strong>: 서버리스 데이터 웨어하우스</li>
<li><strong>Dataproc</strong>: Hadoop/Spark 클러스터 관리</li>
<li><strong>Dataflow</strong>: 스트리밍/배치 처리 파이프라인</li>
</ul>
<h3 id="🔹-azure">🔹 Azure</h3>
<ul>
<li><strong>HDInsight</strong>: Hadoop, Spark 기반 분석 플랫폼</li>
<li><strong>Synapse Analytics</strong>: DW + 빅데이터 통합 분석 플랫폼</li>
<li><strong>Data Factory</strong>: ETL/데이터 흐름 자동화</li>
</ul>
<hr />
<h2 id="📌-4-데이터-분석-도구-jupyter-python-r">📌 4. 데이터 분석 도구: Jupyter, Python, R</h2>
<h3 id="🔹-jupyter-notebook">🔹 Jupyter Notebook</h3>
<ul>
<li>웹 기반 인터랙티브 분석 환경</li>
<li>시각화, 코드, 결과, Markdown 문서를 한 번에 정리</li>
</ul>
<h3 id="🔹-python">🔹 Python</h3>
<ul>
<li>Pandas, NumPy, Scikit-learn, Matplotlib 등 <strong>데이터 분석 생태계 풍부</strong><pre><code class="language-python">import pandas as pd
import matplotlib.pyplot as plt
</code></pre>
</li>
</ul>
<p>df = pd.read_csv(&quot;data.csv&quot;)
df[&quot;value&quot;].hist()
plt.show()</p>
<pre><code>
### 🔹 R
- 통계 분석과 시각화에 강력한 언어
- tidyverse, ggplot2, caret 패키지 활용

```R
library(ggplot2)
data(mpg)
ggplot(mpg, aes(displ, hwy)) + geom_point()</code></pre>