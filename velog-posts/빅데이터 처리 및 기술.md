<h1 id="3-빅데이터-처리-및-기술">3. 빅데이터 처리 및 기술</h1>
<h2 id="📌-1-분산-처리의-개념">📌 1. 분산 처리의 개념</h2>
<h3 id="🔹-정의">🔹 정의</h3>
<ul>
<li>하나의 큰 작업을 여러 대의 컴퓨터(노드)에 나눠서 병렬로 처리하는 방식</li>
<li>데이터 양이 많거나 처리 시간이 긴 경우 유용</li>
</ul>
<h3 id="🔹-필요성">🔹 필요성</h3>
<ul>
<li>데이터의 양이 급증 → 단일 시스템 처리 한계</li>
<li>장애 발생 시 일부 노드로 복구 가능 (내결함성)</li>
<li>성능 향상 및 확장성 확보</li>
</ul>
<h3 id="🔹-대표-기술">🔹 대표 기술</h3>
<ul>
<li>Hadoop, Spark, Kafka 등</li>
</ul>
<hr />
<h2 id="📌-2-mapreduce-원리">📌 2. MapReduce 원리</h2>
<h3 id="🔹-mapreduce란">🔹 MapReduce란?</h3>
<ul>
<li>Google에서 개발한 <strong>대용량 데이터 처리 모델</strong></li>
<li><strong>Map(분할) → Shuffle(정렬) → Reduce(집계)</strong> 순서로 병렬 처리</li>
</ul>
<h3 id="🔹-처리-구조-예시-단어-개수-세기">🔹 처리 구조 예시 (단어 개수 세기)</h3>
<ol>
<li><strong>Map 단계</strong>: (&quot;apple&quot;, 1), (&quot;banana&quot;, 1), (&quot;apple&quot;, 1)</li>
<li><strong>Shuffle 단계</strong>: 키 기준 정렬 → &quot;apple&quot;: [1,1], &quot;banana&quot;: [1]</li>
<li><strong>Reduce 단계</strong>: &quot;apple&quot;: 2, &quot;banana&quot;: 1</li>
</ol>
<pre><code class="language-python"># 의사 코드 형태의 MapReduce
map = lambda text: [(word, 1) for word in text.split()]
shuffle = lambda mapped: {k: [v for _, v in group] for k, group in groupby(sorted(mapped), key=lambda x: x[0])}
reduce = lambda shuffled: {k: sum(v) for k, v in shuffled.items()}</code></pre>
<h3 id="🔹-한계">🔹 한계</h3>
<ul>
<li>디스크 기반 처리로 느림</li>
<li>반복 연산(ML 등)에 부적합</li>
</ul>
<hr />
<h2 id="📌-3-spark-구조와-장점">📌 3. Spark 구조와 장점</h2>
<h3 id="🔹-apache-spark란">🔹 Apache Spark란?</h3>
<ul>
<li><strong>인메모리 기반의 분산 데이터 처리 프레임워크</strong></li>
<li>대규모 데이터 연산을 빠르게 수행 (MapReduce 대비 최대 100배 빠름)</li>
</ul>
<h3 id="🔹-주요-구성-요소">🔹 주요 구성 요소</h3>
<ul>
<li><strong>Driver</strong>: 실행 계획 수립</li>
<li><strong>Executor</strong>: 실제 작업 처리 노드</li>
<li><strong>RDD (Resilient Distributed Dataset)</strong>: 분산 데이터 컬렉션 (불변, 병렬 처리 가능)</li>
</ul>
<pre><code class="language-python">from pyspark import SparkContext

sc = SparkContext()
words = sc.parallelize([&quot;apple&quot;, &quot;banana&quot;, &quot;apple&quot;])
counts = words.map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b)
print(counts.collect())</code></pre>
<h3 id="🔹-spark의-장점">🔹 Spark의 장점</h3>
<ul>
<li>메모리 기반 처리 (빠름)</li>
<li>다양한 언어 지원 (Python, Scala, Java)</li>
<li>실시간 스트리밍, ML, SQL 처리 지원</li>
</ul>
<hr />
<h2 id="📌-4-etl-extract-transform-load">📌 4. ETL (Extract, Transform, Load)</h2>
<h3 id="🔹-정의-1">🔹 정의</h3>
<ul>
<li>데이터를 <strong>가져오고(Extract) → 가공하고(Transform) → 저장하는(Load)</strong> 일련의 처리 과정</li>
</ul>
<table>
<thead>
<tr>
<th>단계</th>
<th>설명</th>
<th>예시</th>
</tr>
</thead>
<tbody><tr>
<td>Extract</td>
<td>다양한 원천에서 데이터 수집</td>
<td>DB, API, 파일 등</td>
</tr>
<tr>
<td>Transform</td>
<td>형식 변환, 결측치 처리, 필터링</td>
<td>Pandas, PySpark 등으로 가공</td>
</tr>
<tr>
<td>Load</td>
<td>분석/저장 시스템으로 적재</td>
<td>DW, Data Lake, RDB 등</td>
</tr>
</tbody></table>
<h3 id="🔹-도구">🔹 도구</h3>
<ul>
<li>Airflow, Luigi, Talend, SSIS 등</li>
</ul>
<hr />
<h2 id="📌-5-데이터-파이프라인-구성">📌 5. 데이터 파이프라인 구성</h2>
<h3 id="🔹-정의-2">🔹 정의</h3>
<ul>
<li><strong>데이터 수집 → 처리 → 저장 → 시각화</strong>까지 자동으로 흐르게 만드는 처리 경로</li>
</ul>
<h3 id="🔹-구성-요소">🔹 구성 요소</h3>
<ol>
<li><strong>수집 단계</strong>: Kafka, API, 센서, 로그 등</li>
<li><strong>처리 단계</strong>: Spark, Flink, Python ETL 코드</li>
<li><strong>저장 단계</strong>: HDFS, S3, MongoDB 등</li>
<li><strong>활용 단계</strong>: Tableau, PowerBI, 머신러닝 모델 등</li>
</ol>
<h3 id="🔹-예시-구조">🔹 예시 구조</h3>
<pre><code>[웹 로그 수집] → Kafka → Spark Streaming 처리 → S3 저장 → Tableau 시각화</code></pre>