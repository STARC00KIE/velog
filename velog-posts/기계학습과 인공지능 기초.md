<h1 id="5-기계학습과-인공지능-기초">5. 기계학습과 인공지능 기초</h1>
<h2 id="📌-1-머신러닝-개념과-유형">📌 1. 머신러닝 개념과 유형</h2>
<h3 id="🔹-머신러닝machine-learning이란">🔹 머신러닝(Machine Learning)이란?</h3>
<ul>
<li><strong>명시적인 규칙 없이</strong>, 데이터로부터 스스로 학습하여 예측하거나 분류하는 알고리즘</li>
<li>인공지능(AI)의 한 분야</li>
</ul>
<h3 id="🔹-머신러닝의-유형">🔹 머신러닝의 유형</h3>
<table>
<thead>
<tr>
<th>유형</th>
<th>설명</th>
<th>예시</th>
</tr>
</thead>
<tbody><tr>
<td>지도학습</td>
<td>입력과 정답(Label)을 기반으로 학습</td>
<td>분류, 회귀</td>
</tr>
<tr>
<td>비지도학습</td>
<td>정답 없이 구조나 패턴을 학습</td>
<td>군집화, 차원축소</td>
</tr>
<tr>
<td>강화학습</td>
<td>보상 기반의 학습 (Trial &amp; Error)</td>
<td>게임, 로봇 제어</td>
</tr>
</tbody></table>
<hr />
<h2 id="📌-2-주요-알고리즘">📌 2. 주요 알고리즘</h2>
<h3 id="🔹-knn-k-nearest-neighbors">🔹 KNN (K-Nearest Neighbors)</h3>
<ul>
<li>주변의 k개 데이터를 참조하여 다수결로 분류<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)</code></pre>
</li>
</ul>
<h3 id="🔹-svm-support-vector-machine">🔹 SVM (Support Vector Machine)</h3>
<ul>
<li>클래스 간 <strong>마진이 최대화되는 결정 경계</strong>를 찾는 알고리즘</li>
<li>커널 함수를 통해 비선형 분리 가능</li>
</ul>
<h3 id="🔹-결정트리--랜덤포레스트">🔹 결정트리 / 랜덤포레스트</h3>
<ul>
<li><strong>결정트리</strong>: 조건 분기를 통해 예측 (직관적)</li>
<li><strong>랜덤포레스트</strong>: 여러 트리를 앙상블 → 과적합 감소, 정확도 향상<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)</code></pre>
</li>
</ul>
<h3 id="🔹-xgboost">🔹 XGBoost</h3>
<ul>
<li>Gradient Boosting 기반 고성능 모델</li>
<li>빠른 연산 + 과적합 방지 + 특성 중요도 제공<pre><code class="language-python">from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train, y_train)</code></pre>
</li>
</ul>
<hr />
<h2 id="📌-3-군집화-clustering">📌 3. 군집화 (Clustering)</h2>
<h3 id="🔹-k-means">🔹 K-means</h3>
<ul>
<li>군집 수(k)를 지정 → 중심점과의 거리 기반으로 군집화<pre><code class="language-python">from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(data)
print(kmeans.labels_)</code></pre>
</li>
</ul>
<h3 id="🔹-dbscan-계층적-군집-등도-활용-가능">🔹 DBSCAN, 계층적 군집 등도 활용 가능</h3>
<hr />
<h2 id="📌-4-연관-규칙-학습">📌 4. 연관 규칙 학습</h2>
<h3 id="🔹-apriori-알고리즘">🔹 Apriori 알고리즘</h3>
<ul>
<li>장바구니 분석처럼 <strong>항목 간 연관성 규칙</strong> 추출</li>
<li>support, confidence, lift 등 기준 사용</li>
</ul>
<pre><code class="language-python">from mlxtend.frequent_patterns import apriori, association_rules
frequent = apriori(df, min_support=0.2, use_colnames=True)
rules = association_rules(frequent, metric=&quot;confidence&quot;, min_threshold=0.7)
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])</code></pre>
<hr />
<h2 id="📌-5-모델-성능-평가-지표">📌 5. 모델 성능 평가 지표</h2>
<table>
<thead>
<tr>
<th>지표</th>
<th>설명</th>
</tr>
</thead>
<tbody><tr>
<td>Accuracy</td>
<td>전체 중 맞춘 비율</td>
</tr>
<tr>
<td>Precision</td>
<td>Positive 예측 중 실제로 Positive 비율 (정밀도)</td>
</tr>
<tr>
<td>Recall</td>
<td>실제 Positive 중 맞춘 비율 (재현율)</td>
</tr>
<tr>
<td>F1 Score</td>
<td>Precision과 Recall의 조화 평균</td>
</tr>
<tr>
<td>ROC-AUC</td>
<td>분류 임계값 변화에 따른 민감도-특이도 그래프 면적</td>
</tr>
</tbody></table>
<pre><code class="language-python">from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))</code></pre>
<hr />
<h2 id="📌-6-과적합--과소적합">📌 6. 과적합 / 과소적합</h2>
<h3 id="🔹-과적합-overfitting">🔹 과적합 (Overfitting)</h3>
<ul>
<li>학습 데이터에 너무 특화 → 테스트 데이터 성능 저하</li>
<li>해결: Regularization, Dropout, 데이터 증강 등</li>
</ul>
<h3 id="🔹-과소적합-underfitting">🔹 과소적합 (Underfitting)</h3>
<ul>
<li>모델이 너무 단순 → 학습도, 예측도 모두 낮음</li>
<li>해결: 더 복잡한 모델 사용, 변수 추가 등</li>
</ul>
<hr />
<h2 id="📌-7-교차검증-cross-validation">📌 7. 교차검증 (Cross Validation)</h2>
<h3 id="🔹-개념">🔹 개념</h3>
<ul>
<li>데이터를 여러 부분으로 나눠 <strong>여러 번 학습/검증</strong> 반복 → 모델의 일반화 성능 평가</li>
</ul>
<h3 id="🔹-k-fold-예시">🔹 K-Fold 예시</h3>
<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
scores = cross_val_score(model, X, y, cv=5)
print(&quot;평균 정확도:&quot;, scores.mean())</code></pre>