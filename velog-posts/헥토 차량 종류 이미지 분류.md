<h1 id="0528---헥토-차량-종류-이미지-분류">05/28 - 헥토 차량 종류 이미지 분류</h1>
<h2 id="1-데이터셋">1. 데이터셋</h2>
<ul>
<li>Class 많음 (클래스당 84개 이미지)</li>
<li>이미지에 노이즈가 존재함</li>
<li>차량이 여러 각도에서 촬영됨</li>
<li>한 클래스 안에 차량 색깔이 여러가지</li>
<li>이미지 사이즈가 500*500 정도</li>
</ul>
<h2 id="2-현재-상황">2. 현재 상황</h2>
<ul>
<li>정규화만 진행한 것보다 데이터 증강까지 진행하고 학습 돌린 결과가 좋음</li>
<li>이미지 사이즈를 224로 학습을 진행한 것보다 384로 진행했을 때 성능이 좋음(resnet18에서 avgloss가 0.05 정도 감소함)</li>
<li>이미지를 Grayscale로 바꾸고 학습을 진행했을 때, 원본 데이터셋으로 학습한 것보다 결과가 좋지 않음</li>
</ul>
<h2 id="3-고려할-점문제점">3. 고려할 점(문제점)</h2>
<ul>
<li>증강을 진행했을 때 학습 속도가 느려짐(성능은 좋아짐)</li>
<li>이미지 사이즈를 384로 고정하고 학습을 진행했을 때 속도가 현저히 느려짐(약 2.9배 정도, 학습하는 에폭 수 증가)</li>
<li>활용 가능한 자원이 별로 좋지 않음(1650 super 6개, 2070 super 1개)</li>
</ul>
<h2 id="4-실험-계획-고찰">4. 실험 계획 고찰</h2>
<ul>
<li>데이터셋을 보면 Fine-Grained Image Classification 이 우리가 목표로 해야 할 방향</li>
<li>현재 우리 데이터셋이랑 가장 비슷한 데이터셋이 Stanford Cars 데이터셋인데 SOTA모델들이 존재하고 코드까지 구현되어 있음</li>
<li>하지만 활용할 수 있는 자원이 별로 좋지 않기 때문에 활용하지 못할 수 있음</li>
<li>따라서 가벼운 모델로 학습 후 앙상블 기법(보팅, 부스팅, 스태킹)을 사용해 결과를 내야 할 수도 있음</li>
</ul>
<h2 id="5-fine-grained-image-classification">5. Fine-Grained Image Classification</h2>
<ul>
<li>서로 유사한 서브 클래스들 간의 미세한 시각적 차이를 정밀하게 구분하는 분류 작업</li>
<li>주요 특징</li>
</ul>
<table>
<thead>
<tr>
<th>특징</th>
<th>설명</th>
</tr>
</thead>
<tbody><tr>
<td><strong>클래스 간 시각적 유사성 높음</strong></td>
<td>대부분의 차량, 새, 꽃, 개 등에서 발생</td>
</tr>
<tr>
<td><strong>정답 클래스 간 차이 미세</strong></td>
<td>패턴, 모양, 각도, 텍스처 수준의 차이</td>
</tr>
<tr>
<td><strong>데이터 수집과 라벨링 어려움</strong></td>
<td>전문가가 필요하거나 라벨 오류 발생 가능</td>
</tr>
<tr>
<td><strong>세밀한 특징 추출 필요</strong></td>
<td>Global 특징 외에 Local/Part 기반 정보 중요</td>
</tr>
<tr>
<td>- 모델 선택 전략</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>전략</th>
<th>설명</th>
<th>추천 모델</th>
</tr>
</thead>
<tbody><tr>
<td><strong>기본 CNN + High Res</strong></td>
<td>큰 해상도 입력 → 미세 특징 보존</td>
<td>ConvNeXt, EfficientNet</td>
</tr>
<tr>
<td><strong>Attention 기반</strong></td>
<td>특징 map에서 중요한 부분에 집중</td>
<td>Swin Transformer, ViT</td>
</tr>
<tr>
<td><strong>Part-based Model</strong></td>
<td>물체 일부 부위 학습 (예: 차량 앞모습)</td>
<td>PCB, PA-CNN, MA-CNN 등</td>
</tr>
<tr>
<td><strong>Multi-Branch</strong></td>
<td>전체 + 부위별 병렬 학습</td>
<td>B-CNN, TASN 등</td>
</tr>
<tr>
<td>- 프로젝트와 연관지어 생각해볼 항목</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>항목</th>
<th>제안</th>
</tr>
</thead>
<tbody><tr>
<td>클래스 간 시각적 차이 작음</td>
<td>반드시 고해상도(384↑) 실험 포함</td>
</tr>
<tr>
<td>촬영 각도 다양</td>
<td>상블 혹은 attention</td>
</tr>
<tr>
<td>자원이 부족</td>
<td>라이트 모델 사용하여 앙상블</td>
</tr>
<tr>
<td>Augmentation 중요</td>
<td>차량 특징을 해치지 않는 선에서 다양하게</td>
</tr>
</tbody></table>
<h2 id="6-앙상블">6. 앙상블</h2>
<ul>
<li>여러 모델을 조합해 최종 예측을 도출하는 방법</li>
<li>다양한 관점(모델)의 의견을 모아 더 신뢰도 높은 예측을 만드는 방식</li>
<li>앙상블 사용 이유<ul>
<li>단일 모델의 한계 극복 (과적합, 편향, 불안정성 등)</li>
<li>모델들이 서로 다른 실수를 보완</li>
<li>일반화 성능(새 데이터에 대한 성능) 향상</li>
</ul>
</li>
<li>주로 사용하는앙상블 기법<ul>
<li>Soft Voting: 분류 문제에 사용(클래스별 확률 평균 중 가장 높은 확률의 클래스 선택)</li>
<li>stacking: 서로 다른 모델들의 예측값을 새로운 입력(feature)로 사용해서, 메타 모델(meta learner, MLP, LightGBM등)이 최종 예측</li>
</ul>
</li>
<li>이미지 분류에서 사용하는 앙상블 전략<ul>
<li>동일 구조, 다른 학습 세트로 학습한 모델들 앙상블 (ex. ConvNeXt 여러 개)</li>
<li>서로 다른 백본 모델 조합 (ex. ResNet + ViT)</li>
<li>하나는 224 입력, 다른 하나는 384 입력 → 해상도 차이를 이용한 다양성 확보</li>
<li>soft voting or stacking 활용</li>
</ul>
</li>
<li>앙상블은 다양성이 핵심! 성능이 비슷한 모델 여러 개를 합치기보다, 다른 방식으로 학습된 모델을 조합해야 더 큰 효과를 봄</li>
</ul>
<h2 id="7-앙상블에-사용할-수-있는-모델">7. 앙상블에 사용할 수 있는 모델</h2>
<ul>
<li>1650 기준</li>
</ul>
<table>
<thead>
<tr>
<th>모델</th>
<th>특징</th>
<th>사용 이유</th>
</tr>
</thead>
<tbody><tr>
<td><strong>TResNet-M or L</strong></td>
<td>FGIR에 최적화</td>
<td>Anti-noise 기반 모델과 호환</td>
</tr>
<tr>
<td><strong>EfficientNet-B3/B4</strong></td>
<td>고해상도용</td>
<td>정확도 + 일반화력 좋음</td>
</tr>
<tr>
<td><strong>ConvNeXt-Base</strong></td>
<td>고성능 CNN</td>
<td>ViT 대안으로 우수</td>
</tr>
<tr>
<td><strong>ResNeXt-101</strong></td>
<td>다양한 경로 학습</td>
<td>일반 이미지 분류 성능 우수</td>
</tr>
<tr>
<td><strong>ViT-B/16 (Vision Transformer)</strong></td>
<td>Transformer 기반</td>
<td>앙상블 다양성 확보 가능</td>
</tr>
<tr>
<td>- 2070s 기준</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>모델</th>
<th>특징</th>
<th>사용 이유</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Swin Transformer-Tiny</strong></td>
<td>Local + Global window attention</td>
<td>각도/구조 변화에 강함</td>
</tr>
<tr>
<td><strong>ML-Decoder + ResNet50</strong></td>
<td>확장 가능한 분류기 구조</td>
<td>클래스 수 많을 때 유리</td>
</tr>
<tr>
<td><strong>PMD (Anti-noise FGIR)</strong></td>
<td>Teacher-Student 기반</td>
<td>노이즈에 강하고 Fine-Grained 성능 우수</td>
</tr>
<tr>
<td><strong>CLIP (ALIGN 변형)</strong></td>
<td>텍스트 연계 가능</td>
<td>Zero-shot 시도 가능 or Multi-modal Voting</td>
</tr>
</tbody></table>
<h2 id="8-stanford-image-dataset-sotafgir-모델">8. stanford image dataset SOTA(FGIR) 모델</h2>
<ul>
<li>코드 존재하는 것 5개 정리</li>
<li>TResNet-L + PMD: 실전성과 성능 균형 최고, 노이즈에 강함</li>
<li>CMAL-Net: 레이어 간 정보 공유로 더 세밀한 차이 학습</li>
<li>I2-HOFI: 특징 상호작용 → 더 깊은 시각적 이해</li>
<li>ML-Decoder: 다양한 클래스에 유연한 분류기</li>
<li>ALIGN: 텍스트와 이미지 동시에 활용하는 거대 모델</li>
</ul>
<h2 id="9-sota-모델들-pretrained-weight-존재-여부">9. SOTA 모델들 pretrained weight 존재 여부</h2>
<table>
<thead>
<tr>
<th>모델</th>
<th>Pretrained 있음?</th>
<th>학습 필요 여부</th>
<th>비고</th>
</tr>
</thead>
<tbody><tr>
<td>TResNet-L + PMD</td>
<td>O</td>
<td>X (Inference 바로 가능)</td>
<td>weightsFromCloud 폴더</td>
</tr>
<tr>
<td>CMAL-Net</td>
<td>O</td>
<td>X (추론 가능)</td>
<td><code>checkpoints/</code> 내 pth</td>
</tr>
<tr>
<td>I2-HOFI</td>
<td>O</td>
<td>X</td>
<td>완성된 모델 .pth 제공</td>
</tr>
<tr>
<td>ML-Decoder</td>
<td>O</td>
<td>O 일부 학습 필요</td>
<td>백본만 pretrained로 시작 가능</td>
</tr>
<tr>
<td>ALIGN</td>
<td>O</td>
<td>O (Zero-shot or fine-tune)</td>
<td>CLIP 변형 활용</td>
</tr>
</tbody></table>
<h2 id="10-고려할-점">10. 고려할 점</h2>
<table>
<thead>
<tr>
<th>항목</th>
<th>주의사항</th>
</tr>
</thead>
<tbody><tr>
<td>이미지 크기</td>
<td>500x500 → 384로 줄이되 정보 손실 주의</td>
</tr>
<tr>
<td>클래스 불균형</td>
<td>Stratified Sampling + BalancedSampler 필요</td>
</tr>
<tr>
<td>데이터 적음</td>
<td>TTA, Multi-crop, CutMix 등 필수 활용</td>
</tr>
<tr>
<td>모델 자원</td>
<td>1650 GPU → <code>TResNet-M</code> 또는 <code>EfficientNet-B3</code>로 경량화 가능</td>
</tr>
</tbody></table>