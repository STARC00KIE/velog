<h1 id="5-ê¸°ê³„í•™ìŠµê³¼-ì¸ê³µì§€ëŠ¥-ê¸°ì´ˆ">5. ê¸°ê³„í•™ìŠµê³¼ ì¸ê³µì§€ëŠ¥ ê¸°ì´ˆ</h1>
<h2 id="ğŸ“Œ-1-ë¨¸ì‹ ëŸ¬ë‹-ê°œë…ê³¼-ìœ í˜•">ğŸ“Œ 1. ë¨¸ì‹ ëŸ¬ë‹ ê°œë…ê³¼ ìœ í˜•</h2>
<h3 id="ğŸ”¹-ë¨¸ì‹ ëŸ¬ë‹machine-learningì´ë€">ğŸ”¹ ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì´ë€?</h3>
<ul>
<li><strong>ëª…ì‹œì ì¸ ê·œì¹™ ì—†ì´</strong>, ë°ì´í„°ë¡œë¶€í„° ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡í•˜ê±°ë‚˜ ë¶„ë¥˜í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜</li>
<li>ì¸ê³µì§€ëŠ¥(AI)ì˜ í•œ ë¶„ì•¼</li>
</ul>
<h3 id="ğŸ”¹-ë¨¸ì‹ ëŸ¬ë‹ì˜-ìœ í˜•">ğŸ”¹ ë¨¸ì‹ ëŸ¬ë‹ì˜ ìœ í˜•</h3>
<table>
<thead>
<tr>
<th>ìœ í˜•</th>
<th>ì„¤ëª…</th>
<th>ì˜ˆì‹œ</th>
</tr>
</thead>
<tbody><tr>
<td>ì§€ë„í•™ìŠµ</td>
<td>ì…ë ¥ê³¼ ì •ë‹µ(Label)ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ</td>
<td>ë¶„ë¥˜, íšŒê·€</td>
</tr>
<tr>
<td>ë¹„ì§€ë„í•™ìŠµ</td>
<td>ì •ë‹µ ì—†ì´ êµ¬ì¡°ë‚˜ íŒ¨í„´ì„ í•™ìŠµ</td>
<td>êµ°ì§‘í™”, ì°¨ì›ì¶•ì†Œ</td>
</tr>
<tr>
<td>ê°•í™”í•™ìŠµ</td>
<td>ë³´ìƒ ê¸°ë°˜ì˜ í•™ìŠµ (Trial &amp; Error)</td>
<td>ê²Œì„, ë¡œë´‡ ì œì–´</td>
</tr>
</tbody></table>
<hr />
<h2 id="ğŸ“Œ-2-ì£¼ìš”-ì•Œê³ ë¦¬ì¦˜">ğŸ“Œ 2. ì£¼ìš” ì•Œê³ ë¦¬ì¦˜</h2>
<h3 id="ğŸ”¹-knn-k-nearest-neighbors">ğŸ”¹ KNN (K-Nearest Neighbors)</h3>
<ul>
<li>ì£¼ë³€ì˜ kê°œ ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹¤ìˆ˜ê²°ë¡œ ë¶„ë¥˜<pre><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)</code></pre>
</li>
</ul>
<h3 id="ğŸ”¹-svm-support-vector-machine">ğŸ”¹ SVM (Support Vector Machine)</h3>
<ul>
<li>í´ë˜ìŠ¤ ê°„ <strong>ë§ˆì§„ì´ ìµœëŒ€í™”ë˜ëŠ” ê²°ì • ê²½ê³„</strong>ë¥¼ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜</li>
<li>ì»¤ë„ í•¨ìˆ˜ë¥¼ í†µí•´ ë¹„ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥</li>
</ul>
<h3 id="ğŸ”¹-ê²°ì •íŠ¸ë¦¬--ëœë¤í¬ë ˆìŠ¤íŠ¸">ğŸ”¹ ê²°ì •íŠ¸ë¦¬ / ëœë¤í¬ë ˆìŠ¤íŠ¸</h3>
<ul>
<li><strong>ê²°ì •íŠ¸ë¦¬</strong>: ì¡°ê±´ ë¶„ê¸°ë¥¼ í†µí•´ ì˜ˆì¸¡ (ì§ê´€ì )</li>
<li><strong>ëœë¤í¬ë ˆìŠ¤íŠ¸</strong>: ì—¬ëŸ¬ íŠ¸ë¦¬ë¥¼ ì•™ìƒë¸” â†’ ê³¼ì í•© ê°ì†Œ, ì •í™•ë„ í–¥ìƒ<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)</code></pre>
</li>
</ul>
<h3 id="ğŸ”¹-xgboost">ğŸ”¹ XGBoost</h3>
<ul>
<li>Gradient Boosting ê¸°ë°˜ ê³ ì„±ëŠ¥ ëª¨ë¸</li>
<li>ë¹ ë¥¸ ì—°ì‚° + ê³¼ì í•© ë°©ì§€ + íŠ¹ì„± ì¤‘ìš”ë„ ì œê³µ<pre><code class="language-python">from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train, y_train)</code></pre>
</li>
</ul>
<hr />
<h2 id="ğŸ“Œ-3-êµ°ì§‘í™”-clustering">ğŸ“Œ 3. êµ°ì§‘í™” (Clustering)</h2>
<h3 id="ğŸ”¹-k-means">ğŸ”¹ K-means</h3>
<ul>
<li>êµ°ì§‘ ìˆ˜(k)ë¥¼ ì§€ì • â†’ ì¤‘ì‹¬ì ê³¼ì˜ ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ êµ°ì§‘í™”<pre><code class="language-python">from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3)
kmeans.fit(data)
print(kmeans.labels_)</code></pre>
</li>
</ul>
<h3 id="ğŸ”¹-dbscan-ê³„ì¸µì -êµ°ì§‘-ë“±ë„-í™œìš©-ê°€ëŠ¥">ğŸ”¹ DBSCAN, ê³„ì¸µì  êµ°ì§‘ ë“±ë„ í™œìš© ê°€ëŠ¥</h3>
<hr />
<h2 id="ğŸ“Œ-4-ì—°ê´€-ê·œì¹™-í•™ìŠµ">ğŸ“Œ 4. ì—°ê´€ ê·œì¹™ í•™ìŠµ</h2>
<h3 id="ğŸ”¹-apriori-ì•Œê³ ë¦¬ì¦˜">ğŸ”¹ Apriori ì•Œê³ ë¦¬ì¦˜</h3>
<ul>
<li>ì¥ë°”êµ¬ë‹ˆ ë¶„ì„ì²˜ëŸ¼ <strong>í•­ëª© ê°„ ì—°ê´€ì„± ê·œì¹™</strong> ì¶”ì¶œ</li>
<li>support, confidence, lift ë“± ê¸°ì¤€ ì‚¬ìš©</li>
</ul>
<pre><code class="language-python">from mlxtend.frequent_patterns import apriori, association_rules
frequent = apriori(df, min_support=0.2, use_colnames=True)
rules = association_rules(frequent, metric=&quot;confidence&quot;, min_threshold=0.7)
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])</code></pre>
<hr />
<h2 id="ğŸ“Œ-5-ëª¨ë¸-ì„±ëŠ¥-í‰ê°€-ì§€í‘œ">ğŸ“Œ 5. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ</h2>
<table>
<thead>
<tr>
<th>ì§€í‘œ</th>
<th>ì„¤ëª…</th>
</tr>
</thead>
<tbody><tr>
<td>Accuracy</td>
<td>ì „ì²´ ì¤‘ ë§ì¶˜ ë¹„ìœ¨</td>
</tr>
<tr>
<td>Precision</td>
<td>Positive ì˜ˆì¸¡ ì¤‘ ì‹¤ì œë¡œ Positive ë¹„ìœ¨ (ì •ë°€ë„)</td>
</tr>
<tr>
<td>Recall</td>
<td>ì‹¤ì œ Positive ì¤‘ ë§ì¶˜ ë¹„ìœ¨ (ì¬í˜„ìœ¨)</td>
</tr>
<tr>
<td>F1 Score</td>
<td>Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· </td>
</tr>
<tr>
<td>ROC-AUC</td>
<td>ë¶„ë¥˜ ì„ê³„ê°’ ë³€í™”ì— ë”°ë¥¸ ë¯¼ê°ë„-íŠ¹ì´ë„ ê·¸ë˜í”„ ë©´ì </td>
</tr>
</tbody></table>
<pre><code class="language-python">from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))</code></pre>
<hr />
<h2 id="ğŸ“Œ-6-ê³¼ì í•©--ê³¼ì†Œì í•©">ğŸ“Œ 6. ê³¼ì í•© / ê³¼ì†Œì í•©</h2>
<h3 id="ğŸ”¹-ê³¼ì í•©-overfitting">ğŸ”¹ ê³¼ì í•© (Overfitting)</h3>
<ul>
<li>í•™ìŠµ ë°ì´í„°ì— ë„ˆë¬´ íŠ¹í™” â†’ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ ì €í•˜</li>
<li>í•´ê²°: Regularization, Dropout, ë°ì´í„° ì¦ê°• ë“±</li>
</ul>
<h3 id="ğŸ”¹-ê³¼ì†Œì í•©-underfitting">ğŸ”¹ ê³¼ì†Œì í•© (Underfitting)</h3>
<ul>
<li>ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœ â†’ í•™ìŠµë„, ì˜ˆì¸¡ë„ ëª¨ë‘ ë‚®ìŒ</li>
<li>í•´ê²°: ë” ë³µì¡í•œ ëª¨ë¸ ì‚¬ìš©, ë³€ìˆ˜ ì¶”ê°€ ë“±</li>
</ul>
<hr />
<h2 id="ğŸ“Œ-7-êµì°¨ê²€ì¦-cross-validation">ğŸ“Œ 7. êµì°¨ê²€ì¦ (Cross Validation)</h2>
<h3 id="ğŸ”¹-ê°œë…">ğŸ”¹ ê°œë…</h3>
<ul>
<li>ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ  <strong>ì—¬ëŸ¬ ë²ˆ í•™ìŠµ/ê²€ì¦</strong> ë°˜ë³µ â†’ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€</li>
</ul>
<h3 id="ğŸ”¹-k-fold-ì˜ˆì‹œ">ğŸ”¹ K-Fold ì˜ˆì‹œ</h3>
<pre><code class="language-python">from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
scores = cross_val_score(model, X, y, cv=5)
print(&quot;í‰ê·  ì •í™•ë„:&quot;, scores.mean())</code></pre>